import { useState, useRef, useEffect } from "react";
import { Mic, Loader2, Type, Sparkles } from "lucide-react";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { useToast } from "@/hooks/use-toast";
import { supabase } from "@/integrations/supabase/client";
import { motion } from "framer-motion";
import Confetti from "react-confetti";
import { z } from "zod";

const expenseInputSchema = z.object({
  text: z.string()
    .trim()
    .min(1, { message: "è«‹è¼¸å…¥æ¶ˆè²»è¨˜éŒ„" })
    .max(500, { message: "è¼¸å…¥å…§å®¹éé•·ï¼Œè«‹é™åˆ¶åœ¨ 500 å­—ä»¥å…§" })
    .refine(
      (text) => text.length > 5,
      { message: "è«‹è¼¸å…¥æ›´è©³ç´°çš„æ¶ˆè²»è¨˜éŒ„ï¼Œä¾‹å¦‚ï¼šåœ¨æ˜Ÿå·´å…‹èŠ±äº†150å…ƒè²·å’–å•¡" }
    )
});

interface VoiceInputProps {
  onExpenseCreated: () => void;
}

const VoiceInput = ({ onExpenseCreated }: VoiceInputProps) => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [recognizedText, setRecognizedText] = useState("");
  const [interimText, setInterimText] = useState("");
  const [showManualInput, setShowManualInput] = useState(false);
  const [manualText, setManualText] = useState("");
  const [useRecording, setUseRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null);
  const [audioChunks, setAudioChunks] = useState<Blob[]>([]);
  const [showConfetti, setShowConfetti] = useState(false);
  const [windowSize, setWindowSize] = useState({ width: window.innerWidth, height: window.innerHeight });
  const { toast } = useToast();
  const recognitionRef = useRef<any>(null);
  const timeoutRef = useRef<NodeJS.Timeout | null>(null);
  const currentLanguageIndex = useRef(0);

  useEffect(() => {
    const handleResize = () => {
      setWindowSize({ width: window.innerWidth, height: window.innerHeight });
    };
    window.addEventListener('resize', handleResize);
    return () => window.removeEventListener('resize', handleResize);
  }, []);

  const preprocessText = (text: string): string => {
    let processed = text.trim();
    processed = processed.replace(/[å—¯å‘ƒå•Šå–”å“¦]/g, '');
    processed = processed.replace(/å¡ŠéŒ¢/g, 'å…ƒ');
    processed = processed.replace(/([0-9]+)å¡Š/g, '$1å…ƒ');
    processed = processed.replace(/ä»Šæ—¥|ä»Šä»”æ—¥/g, 'ä»Šå¤©');
    processed = processed.replace(/æ˜¨æ—¥|æ˜¨ä»”æ—¥/g, 'æ˜¨å¤©');
    return processed;
  };

  const startRecording = async () => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      toast({
        title: "ä¸æ”¯æ´èªéŸ³è¼¸å…¥",
        description: "æ‚¨çš„ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¾¨è­˜åŠŸèƒ½ï¼Œè«‹æ”¹ç”¨æ–‡å­—è¼¸å…¥",
        variant: "destructive",
      });
      setShowManualInput(true);
      return;
    }

    try {
      await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('Microphone permission granted');
    } catch (error: any) {
      console.error('Microphone permission error:', error);
      toast({
        title: "éœ€è¦éº¥å…‹é¢¨æ¬Šé™",
        description: "è«‹å…è¨±ä½¿ç”¨éº¥å…‹é¢¨ï¼Œæˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥",
        variant: "destructive",
      });
      setShowManualInput(true);
      return;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    recognitionRef.current = new SpeechRecognition();
    
    const languages = ['zh-CN', 'zh-HK', 'zh-TW', 'zh', 'en-US'];
    recognitionRef.current.lang = languages[currentLanguageIndex.current];
    
    recognitionRef.current.continuous = false;
    recognitionRef.current.interimResults = true;
    recognitionRef.current.maxAlternatives = 3;
    
    console.log('Starting speech recognition with language:', recognitionRef.current.lang);

    timeoutRef.current = setTimeout(() => {
      if (isRecording && recognitionRef.current) {
        console.log('Speech timeout - no speech detected');
        recognitionRef.current.stop();
        toast({
          title: "æœªåµæ¸¬åˆ°èªéŸ³",
          description: "è«‹å†è©¦ä¸€æ¬¡ï¼Œæˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥",
        });
        setShowManualInput(true);
      }
    }, 10000);

    recognitionRef.current.onstart = () => {
      setIsRecording(true);
      setRecognizedText("");
      setInterimText("");
      console.log('Speech recognition started');
    };

    recognitionRef.current.onresult = async (event: any) => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }

      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        const confidence = event.results[i][0].confidence;
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
          console.log('Confidence:', confidence);
          
          if (confidence < 0.7) {
            setInterimText(`${transcript} (ç½®ä¿¡åº¦è¼ƒä½ï¼Œè«‹ç¢ºèª)`);
          }
        } else {
          interimTranscript += transcript;
        }
      }

      if (interimTranscript) {
        setInterimText(interimTranscript);
        console.log('Interim text:', interimTranscript);
      }

      if (finalTranscript) {
        console.log('Final recognized text:', finalTranscript);
        const preprocessed = preprocessText(finalTranscript);
        console.log('Preprocessed text:', preprocessed);
        setRecognizedText(preprocessed);
        setInterimText("");
        setIsRecording(false);
        
        await processExpense(preprocessed);
      }
    };

    recognitionRef.current.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error, event);
      
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }

      setIsRecording(false);
      setInterimText("");
      
      if (event.error === 'language-not-supported' && currentLanguageIndex.current < 4) {
        currentLanguageIndex.current++;
        console.log('Retrying with next language...');
        toast({
          title: "åˆ‡æ›èªè¨€ä¸­",
          description: "æ­£åœ¨å˜—è©¦å…¶ä»–èªè¨€è¨­å®š...",
        });
        setTimeout(() => startRecording(), 500);
        return;
      }

      let errorMessage = "è«‹å†è©¦ä¸€æ¬¡ï¼Œæˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥";
      
      switch (event.error) {
        case 'not-allowed':
        case 'service-not-allowed':
          errorMessage = "è«‹å…è¨±ç€è¦½å™¨ä½¿ç”¨éº¥å…‹é¢¨æ¬Šé™ï¼Œæˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥";
          setShowManualInput(true);
          break;
        case 'no-speech':
          errorMessage = "æ²’æœ‰åµæ¸¬åˆ°èªéŸ³ï¼Œè«‹é‡è©¦æˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥";
          setShowManualInput(true);
          break;
        case 'audio-capture':
          errorMessage = "ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥è¨­å‚™æˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥";
          setShowManualInput(true);
          break;
        case 'network':
          errorMessage = "ç¶²è·¯é€£ç·šå•é¡Œï¼Œè«‹æª¢æŸ¥ç¶²è·¯";
          break;
        case 'language-not-supported':
          errorMessage = "èªè¨€ä¸æ”¯æ´ï¼Œè«‹æ”¹ç”¨æ–‡å­—è¼¸å…¥";
          setShowManualInput(true);
          break;
        case 'aborted':
          return;
      }
      
      toast({
        title: "èªéŸ³è¾¨è­˜éŒ¯èª¤",
        description: errorMessage,
        variant: "destructive",
      });
    };

    recognitionRef.current.onend = () => {
      console.log('Speech recognition ended');
      
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }

      if (interimText && !recognizedText && isRecording) {
        console.log('Processing interim text as final:', interimText);
        const preprocessed = preprocessText(interimText);
        setRecognizedText(preprocessed);
        processExpense(preprocessed);
      }
      
      setIsRecording(false);
      setInterimText("");
    };

    recognitionRef.current.start();
  };

  const startAudioRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      const chunks: Blob[] = [];

      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunks.push(e.data);
        }
      };

      recorder.onstop = async () => {
        const audioBlob = new Blob(chunks, { type: 'audio/webm' });
        await transcribeAudio(audioBlob);
        stream.getTracks().forEach(track => track.stop());
      };

      recorder.start();
      setMediaRecorder(recorder);
      setAudioChunks(chunks);
      setIsRecording(true);

      setTimeout(() => {
        if (recorder.state === 'recording') {
          recorder.stop();
          setIsRecording(false);
        }
      }, 15000);

      toast({
        title: "éŒ„éŸ³ä¸­",
        description: "æœ€é•·éŒ„è£½ 15 ç§’ï¼Œè«‹æ¸…æ¥šèªªå‡ºæ¶ˆè²»è¨˜éŒ„",
      });
    } catch (error) {
      console.error('Recording error:', error);
      toast({
        title: "éŒ„éŸ³å¤±æ•—",
        description: "ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œè«‹æ”¹ç”¨æ–‡å­—è¼¸å…¥",
        variant: "destructive",
      });
      setShowManualInput(true);
    }
  };

  const stopAudioRecording = () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop();
      setIsRecording(false);
    }
  };

  const transcribeAudio = async (audioBlob: Blob) => {
    setIsProcessing(true);

    try {
      const reader = new FileReader();
      reader.readAsDataURL(audioBlob);
      
      const base64Audio = await new Promise<string>((resolve, reject) => {
        reader.onload = () => {
          const result = reader.result as string;
          const base64 = result.split(',')[1];
          resolve(base64);
        };
        reader.onerror = reject;
      });

      const { data, error } = await supabase.functions.invoke('transcribe-audio', {
        body: { audio: base64Audio }
      });

      if (error) throw error;

      if (!data.success) {
        throw new Error(data.error || 'Failed to transcribe audio');
      }

      const text = data.text;
      console.log('Transcribed text:', text);
      const preprocessed = preprocessText(text);
      setRecognizedText(preprocessed);

      await processExpense(preprocessed);

    } catch (error: any) {
      console.error('Transcription error:', error);
      
      let errorMessage = "èªéŸ³è½‰æ–‡å­—å¤±æ•—ï¼Œè«‹é‡è©¦æˆ–æ”¹ç”¨æ–‡å­—è¼¸å…¥";
      
      if (error.message?.includes('429')) {
        errorMessage = "ç³»çµ±ç¹å¿™ä¸­ï¼Œè«‹ç¨å¾Œå†è©¦";
      } else if (error.message?.includes('402')) {
        errorMessage = "æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦";
      }
      
      toast({
        title: "è½‰è­¯å¤±æ•—",
        description: errorMessage,
        variant: "destructive",
      });
      setShowManualInput(true);
    } finally {
      setIsProcessing(false);
    }
  };

  const stopRecording = () => {
    if (timeoutRef.current) {
      clearTimeout(timeoutRef.current);
      timeoutRef.current = null;
    }
    
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      setIsRecording(false);
    }
  };

  const processExpense = async (text: string) => {
    setIsProcessing(true);

    try {
      const { data, error } = await supabase.functions.invoke('parse-expense', {
        body: { text }
      });

      if (error) throw error;

      if (!data.success) {
        throw new Error(data.error || 'Failed to parse expense');
      }

      const expense = data.expense;
      console.log('Parsed expense:', expense);

      const { error: insertError } = await supabase
        .from('expenses')
        .insert({
          user_id: (await supabase.auth.getUser()).data.user?.id,
          amount: Number(expense.amount),
          category: expense.category,
          description: expense.description,
          location_name: expense.location_name,
          location_lat: expense.location_lat,
          location_lng: expense.location_lng,
          expense_date: expense.expense_date,
          voice_input: text,
        });

      if (insertError) throw insertError;

      // Track challenges after creating expense
      const { data: { user } } = await supabase.auth.getUser();
      if (user) {
        const { trackAllChallenges: trackChallenges } = await import("@/lib/challengeTracker");
        await trackChallenges(user.id);
      }

      // Celebration!
      setShowConfetti(true);
      setTimeout(() => setShowConfetti(false), 3000);

      if (navigator.vibrate) {
        navigator.vibrate([100, 50, 100]);
      }

      toast({
        title: "ğŸ‰ è¨˜å¸³æˆåŠŸï¼",
        description: `å·²è¨˜éŒ„ ${expense.amount} å…ƒçš„ ${expense.category} æ¶ˆè²»${expense.location_name ? ` - ${expense.location_name}` : ''}`,
      });

      setRecognizedText("");
      setManualText("");
      setShowManualInput(false);
      setUseRecording(false);
      onExpenseCreated();

    } catch (error: any) {
      console.error('Error processing expense:', error);
      
      let errorMessage = error.message || "ç„¡æ³•è™•ç†è¼¸å…¥ï¼Œè«‹å†è©¦ä¸€æ¬¡";
      
      if (error.message?.includes('429')) {
        errorMessage = "ç³»çµ±ç¹å¿™ä¸­ï¼Œè«‹ç¨å¾Œå†è©¦";
      } else if (error.message?.includes('402')) {
        errorMessage = "æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦";
      }
      
      toast({
        title: "è™•ç†å¤±æ•—",
        description: errorMessage,
        variant: "destructive",
      });
    } finally {
      setIsProcessing(false);
    }
  };

  const handleManualSubmit = async () => {
    // Validate input
    const validation = expenseInputSchema.safeParse({ text: manualText });
    if (!validation.success) {
      toast({
        title: "è¼¸å…¥éŒ¯èª¤",
        description: validation.error.errors[0].message,
        variant: "destructive",
      });
      return;
    }
    
    await processExpense(validation.data.text);
  };

  const handleClick = () => {
    if (isRecording) {
      if (useRecording) {
        stopAudioRecording();
      } else {
        stopRecording();
      }
    } else {
      if (!useRecording) {
        startRecording();
      } else {
        startAudioRecording();
      }
    }
  };

  return (
    <>
      {showConfetti && (
        <Confetti
          width={windowSize.width}
          height={windowSize.height}
          recycle={false}
          numberOfPieces={200}
          gravity={0.3}
        />
      )}
      
      <div className="flex flex-col items-center gap-4 w-full max-w-md mx-auto relative">
        {/* Orbit rings */}
        {isRecording && (
          <>
            <motion.div
              className="absolute inset-0 rounded-full border-2 border-primary/30 pointer-events-none"
              style={{ width: '180px', height: '180px', left: 'calc(50% - 90px)', top: '0' }}
              animate={{ rotate: 360 }}
              transition={{ duration: 3, repeat: Infinity, ease: "linear" }}
            />
            <motion.div
              className="absolute inset-0 rounded-full border-2 border-primary/20 pointer-events-none"
              style={{ width: '200px', height: '200px', left: 'calc(50% - 100px)', top: '-10px' }}
              animate={{ rotate: -360 }}
              transition={{ duration: 4, repeat: Infinity, ease: "linear" }}
            />
          </>
        )}

        <motion.div
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
        >
          <Button
            onClick={handleClick}
            disabled={isProcessing}
            size="lg"
            className={`
              relative w-32 h-32 rounded-full shadow-glow-strong overflow-hidden z-10
              transition-spring
              ${isRecording 
                ? 'bg-destructive hover:bg-destructive/90' 
                : isProcessing
                ? 'bg-muted'
                : 'bg-gradient-primary'
              }
              ${!isRecording && !isProcessing && 'animate-breath'}
            `}
          >
            {isProcessing ? (
              <Loader2 className="w-12 h-12 animate-spin" />
            ) : isRecording ? (
              <>
                <motion.div
                  className="absolute inset-0 bg-white/20"
                  animate={{ scale: [1, 1.2, 1] }}
                  transition={{ duration: 1, repeat: Infinity }}
                />
                <Mic className="w-12 h-12 relative z-10" />
              </>
            ) : (
              <>
                <Sparkles className="w-5 h-5 absolute top-3 right-3 text-white/60 animate-pulse" />
                <Mic className="w-12 h-12" />
              </>
            )}
          </Button>
        </motion.div>

        {isRecording && (
          <motion.div 
            className="text-center space-y-2"
            animate={{ opacity: [1, 0.7, 1] }}
            transition={{ duration: 1.5, repeat: Infinity }}
          >
            <p className="text-sm font-medium text-primary">
              ğŸ¤ æ­£åœ¨è†è½ä¸­...ï¼ˆé»æ“Šåœæ­¢ï¼‰
            </p>
            {interimText && (
              <div className="glass-card px-4 py-2 rounded-xl shadow-sm">
                <p className="text-xs text-muted-foreground">å³æ™‚è¾¨è­˜ï¼š</p>
                <p className="text-sm font-medium text-primary">{interimText}</p>
              </div>
            )}
          </motion.div>
        )}

        {isProcessing && (
          <motion.p 
            className="text-sm font-medium text-muted-foreground"
            animate={{ opacity: [1, 0.5, 1] }}
            transition={{ duration: 1, repeat: Infinity }}
          >
            âš¡ è™•ç†ä¸­...
          </motion.p>
        )}

        {!isRecording && !isProcessing && (
          <motion.p
            className="text-sm text-muted-foreground"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            transition={{ delay: 0.2 }}
          >
            é»æ“Šéº¥å…‹é¢¨é–‹å§‹èªéŸ³è¨˜å¸³ âœ¨
          </motion.p>
        )}

        {recognizedText && !isProcessing && (
          <div className="glass-card px-4 py-2 rounded-xl shadow-sm max-w-md">
            <p className="text-xs text-muted-foreground">å·²è¾¨è­˜ï¼š</p>
            <p className="text-sm font-medium">{recognizedText}</p>
          </div>
        )}

        {!showManualInput && (
          <Button
            variant="ghost"
            size="sm"
            onClick={() => setShowManualInput(true)}
            className="text-xs"
          >
            <Type className="w-3 h-3 mr-1" />
            æ”¹ç”¨æ–‡å­—è¼¸å…¥
          </Button>
        )}

        {showManualInput && (
          <motion.div 
            className="flex gap-2 w-full max-w-md"
            initial={{ opacity: 0, y: -10 }}
            animate={{ opacity: 1, y: 0 }}
          >
            <Input
              value={manualText}
              onChange={(e) => setManualText(e.target.value)}
              placeholder="ä¾‹å¦‚ï¼šåœ¨æ˜Ÿå·´å…‹èŠ±äº†150å…ƒè²·å’–å•¡"
              className="flex-1"
              onKeyPress={(e) => {
                if (e.key === 'Enter') {
                  handleManualSubmit();
                }
              }}
            />
            <Button
              onClick={handleManualSubmit}
              disabled={isProcessing || !manualText.trim()}
            >
              é€å‡º
            </Button>
          </motion.div>
        )}
      </div>
    </>
  );
};

export default VoiceInput;
